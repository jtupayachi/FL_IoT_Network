on the server:

[92mINFO [0m:      	 'r2': [(0, 0.0),
[92mINFO [0m:      	        (1, 0.0),
[92mINFO [0m:      	        (2, 0.0),
[92mINFO [0m:      	        (3, 0.0),
[92mINFO [0m:      	        (4, 0.0),
[92mINFO [0m:      	        (5, 0.0),
[92mINFO [0m:      	        (6, 0.0),
[92mINFO [0m:      	        (7, 0.0),
[92mINFO [0m:      	        (8, 0.0),
[92mINFO [0m:      	        (9, 0.0),
[92mINFO [0m:      	        (10, 0.0)],
[92mINFO [0m:      	 'rmse': [(0, 0.0),
[92mINFO [0m:      	          (1, 0.0),
[92mINFO [0m:      	          (2, 0.0),
[92mINFO [0m:      	          (3, 0.0),
[92mINFO [0m:      	          (4, 0.0),
[92mINFO [0m:      	          (5, 0.0),
[92mINFO [0m:      	          (6, 0.0),
[92mINFO [0m:      	          (7, 0.0),
[92mINFO [0m:      	          (8, 0.0),
[92mINFO [0m:      	          (9, 0.0),
[92mINFO [0m:      	          (10, 0.0)]}
[92mINFO [0m:      
🚀 Starting NASA Federated Learning Server
==================================================
Experiment ID: nasa_25c_alpha_0.1_fedavg
Algorithm: FEDAVG
Server: localhost:8686
Rounds: 10
Strategy: fedavg
Min Clients: 25
Results Directory: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251029_110755/nasa_25c_alpha_0.1_fedavg
==================================================
🔄 Starting FL server on localhost:8686...
📊 Round 1 Fit Metrics:
   Clients: 12
   Avg Train Loss: 8864.5635
   Avg Val Loss: 8634.7842
   Avg Val R²: -0.4035
📈 Round 1 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 2 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7193.4177
   Avg Val Loss: 7051.9520
   Avg Val R²: -0.0125
📈 Round 2 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 3 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7211.5512
   Avg Val Loss: 7067.3584
   Avg Val R²: -0.0149
📈 Round 3 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 4 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7205.2089
   Avg Val Loss: 7063.3188
   Avg Val R²: -0.0142
📈 Round 4 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 5 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7189.7140
   Avg Val Loss: 7048.7184
   Avg Val R²: -0.0122
📈 Round 5 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 6 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7204.4143
   Avg Val Loss: 7062.5841
   Avg Val R²: -0.0145
📈 Round 6 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 7 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7199.0073
   Avg Val Loss: 7058.9246
   Avg Val R²: -0.0134
📈 Round 7 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 8 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7203.4937
   Avg Val Loss: 7064.6256
   Avg Val R²: -0.0146
📈 Round 8 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 9 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7200.7270
   Avg Val Loss: 7057.3551
   Avg Val R²: -0.0134
📈 Round 9 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
📊 Round 10 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7198.3268
   Avg Val Loss: 7055.8532
   Avg Val R²: -0.0133
📈 Round 10 Centralized Evaluation:
   Loss: 0.0000
   R²: 0.0000
✅ Server completed successfully!
📊 Results saved to: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251029_110755/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg




import flwr as fl
import torch
import argparse
import json
import os
from typing import Dict, List, Tuple, Optional
import numpy as np
from flwr.common import Parameters, Scalar
from flwr.server.strategy import FedAvg, FedProx
import torch.nn as nn
import pandas as pd
from datetime import datetime
import csv
import sys

class NASAStrategy:
    """Custom strategy wrapper for NASA experiments"""
    
    def __init__(self, strategy_name: str = "fedavg", fraction_fit: float = 1.0, 
                 fraction_evaluate: float = 1.0, min_fit_clients: int = 2,
                 min_evaluate_clients: int = 2, min_available_clients: int = 2,
                 proximal_mu: float = 0.1):
        
        self.strategy_name = strategy_name.lower()
        
        if self.strategy_name == "fedprox":
            self.strategy = FedProx(
                fraction_fit=fraction_fit,
                fraction_evaluate=fraction_evaluate,
                min_fit_clients=min_fit_clients,
                min_evaluate_clients=min_evaluate_clients,
                min_available_clients=min_available_clients,
                proximal_mu=proximal_mu
            )
        else:  # Default to FedAvg
            self.strategy = FedAvg(
                fraction_fit=fraction_fit,
                fraction_evaluate=fraction_evaluate,
                min_fit_clients=min_fit_clients,
                min_evaluate_clients=min_evaluate_clients,
                min_available_clients=min_available_clients
            )
    
    def get_strategy(self):
        return self.strategy

class MetricsCollector:
    """Collects and saves metrics from all rounds"""
    
    def __init__(self, results_dir: str, experiment_id: str):
        self.results_dir = results_dir
        self.experiment_id = experiment_id
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Create results directory
        os.makedirs(results_dir, exist_ok=True)
        
        # Define file paths
        self.round_metrics_path = os.path.join(results_dir, f"round_metrics_{self.timestamp}.csv")
        self.client_metrics_path = os.path.join(results_dir, f"client_metrics_{self.timestamp}.csv")
        self.eval_metrics_path = os.path.join(results_dir, f"eval_metrics_{self.timestamp}.csv")
        
        # Initialize CSV files
        self._initialize_metrics_files()
        
        # Storage for metrics
        self.round_metrics = []
        self.client_metrics = []
        self.eval_metrics = []
    
    def _initialize_metrics_files(self):
        """Initialize CSV files with headers"""
        
        # Round metrics (aggregated per round)
        round_headers = [
            "timestamp", "round", "total_clients", "fit_clients", "eval_clients",
            "avg_train_loss", "avg_val_loss", "avg_train_rmse", "avg_val_rmse",
            "avg_train_r2", "avg_val_r2", "centralized_loss", "algorithm"
        ]
        
        # Client metrics (per client per round)
        client_headers = [
            "timestamp", "round", "client_id", "samples", "train_loss", "val_loss",
            "train_rmse", "val_rmse", "train_r2", "val_r2", "algorithm"
        ]
        
        # Evaluation metrics (centralized evaluation)
        eval_headers = [
            "timestamp", "round", "loss", "rmse", "mse", "mae", "r2", "algorithm"
        ]
        
        # Write headers
        for path, headers in [
            (self.round_metrics_path, round_headers),
            (self.client_metrics_path, client_headers),
            (self.eval_metrics_path, eval_headers)
        ]:
            with open(path, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=headers)
                writer.writeheader()
    
    def collect_fit_metrics(self, round_num: int, results: List[Tuple], failures: List[BaseException]):
        """Collect metrics from fit results"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        total_samples = 0
        train_losses, val_losses = [], []
        train_rmses, val_rmses = [], []
        train_r2s, val_r2s = [], []
        
        for client_proxy, fit_res in results:
            client_id = fit_res.metrics.get("client_id", "unknown")
            num_samples = fit_res.num_examples
            train_loss = fit_res.metrics.get("train_loss", 0)
            val_loss = fit_res.metrics.get("val_loss", 0)
            train_rmse = fit_res.metrics.get("train_rmse", 0)
            val_rmse = fit_res.metrics.get("val_rmse", 0)
            train_r2 = fit_res.metrics.get("train_r2", 0)
            val_r2 = fit_res.metrics.get("val_r2", 0)
            
            # Store client-level metrics
            client_metrics = {
                "timestamp": timestamp,
                "round": round_num,
                "client_id": client_id,
                "samples": num_samples,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "train_rmse": train_rmse,
                "val_rmse": val_rmse,
                "train_r2": train_r2,
                "val_r2": val_r2,
                "algorithm": self.experiment_id
            }
            
            self.client_metrics.append(client_metrics)
            
            # Write immediately to CSV
            with open(self.client_metrics_path, 'a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=client_metrics.keys())
                writer.writerow(client_metrics)
            
            # Accumulate for round averages
            total_samples += num_samples
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_rmses.append(train_rmse)
            val_rmses.append(val_rmse)
            train_r2s.append(train_r2)
            val_r2s.append(val_r2)
        
        # Calculate round averages
        if train_losses:
            round_metrics = {
                "timestamp": timestamp,
                "round": round_num,
                "total_clients": len(results),
                "fit_clients": len(results),
                "eval_clients": 0,  # Will be updated in evaluate
                "avg_train_loss": np.mean(train_losses),
                "avg_val_loss": np.mean(val_losses),
                "avg_train_rmse": np.mean(train_rmses),
                "avg_val_rmse": np.mean(val_rmses),
                "avg_train_r2": np.mean(train_r2s),
                "avg_val_r2": np.mean(val_r2s),
                "centralized_loss": 0,  # Will be updated in evaluate
                "algorithm": self.experiment_id
            }
            
            self.round_metrics.append(round_metrics)
            
            # Write round metrics to CSV
            with open(self.round_metrics_path, 'a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=round_metrics.keys())
                writer.writerow(round_metrics)
            
            print(f"📊 Round {round_num} Fit Metrics:")
            print(f"   Clients: {len(results)}")
            print(f"   Avg Train Loss: {np.mean(train_losses):.4f}")
            print(f"   Avg Val Loss: {np.mean(val_losses):.4f}")
            print(f"   Avg Val R²: {np.mean(val_r2s):.4f}")
    
    def collect_evaluate_metrics(self, round_num: int, loss: float, metrics: Dict[str, Scalar]):
        """Collect metrics from centralized evaluation"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        eval_metrics = {
            "timestamp": timestamp,
            "round": round_num,
            "loss": loss,
            "rmse": metrics.get("rmse", 0),
            "mse": metrics.get("mse", 0),
            "mae": metrics.get("mae", 0),
            "r2": metrics.get("r2", 0),
            "algorithm": self.experiment_id
        }
        
        self.eval_metrics.append(eval_metrics)
        
        # Write eval metrics to CSV
        with open(self.eval_metrics_path, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=eval_metrics.keys())
            writer.writerow(eval_metrics)
        
        # Update round metrics with centralized loss
        if self.round_metrics and len(self.round_metrics) > round_num - 1:
            self.round_metrics[round_num - 1]["centralized_loss"] = loss
            
            # Update the CSV file
            with open(self.round_metrics_path, 'r') as f:
                lines = f.readlines()
            
            if len(lines) > round_num:  # Header + rounds
                # This is a simplified approach - in production you might want a more robust solution
                with open(self.round_metrics_path, 'w') as f:
                    for i, line in enumerate(lines):
                        if i == 0 or i > round_num:
                            f.write(line)
                        else:
                            # Update the specific round line
                            parts = line.strip().split(',')
                            if len(parts) >= 12:  # Ensure we have enough columns
                                parts[11] = str(loss)  # centralized_loss position
                                f.write(','.join(parts) + '\n')
        
        print(f"📈 Round {round_num} Centralized Evaluation:")
        print(f"   Loss: {loss:.4f}")
        print(f"   R²: {metrics.get('r2', 0):.4f}")

def is_server_script():
    """Check if this script is being run as the server"""
    # Check if --client-id is in the arguments (client script)
    return '--client-id' not in ' '.join(sys.argv)

def main():
    """Main function - handles both server and client based on arguments"""
    
    if is_server_script():
        # Server mode - use server argument parser
        server_parser = argparse.ArgumentParser(description="NASA FL Server")
        server_parser.add_argument("--config", type=str, default="config.json", help="Path to config file")
        server_parser.add_argument("--results-dir", type=str, default="results", help="Directory to save results")
        server_parser.add_argument("--port", type=int, help="Server port (overrides config)")
        
        args = server_parser.parse_args()
        run_server(args)
    else:
        # Client mode - let the client script handle it
        print("❌ This script should be run as server only. Use client.py for clients.")
        sys.exit(1)

def run_server(args):
    """Run the FL server"""
    try:
        # Load configuration
        with open(args.config, 'r') as f:
            config = json.load(f)
        
        # Extract server configuration
        server_config = config.get("server", {})
        strategy_config = config.get("strategy", {})
        data_config = config.get("data", {})
        
        # Get server parameters
        server_host = server_config.get("host", "0.0.0.0")
        server_port = args.port or server_config.get("port", 8080)
        num_rounds = server_config.get("num_rounds", 10)
        
        # Get strategy parameters
        strategy_name = strategy_config.get("name", "fedavg")
        fraction_fit = strategy_config.get("fraction_fit", 1.0)
        fraction_evaluate = strategy_config.get("fraction_evaluate", 1.0)
        min_fit_clients = strategy_config.get("min_fit_clients", 2)
        min_evaluate_clients = strategy_config.get("min_evaluate_clients", 2)
        min_available_clients = strategy_config.get("min_available_clients", 
                                                   data_config.get("num_clients", 2))
        proximal_mu = strategy_config.get("proximal_mu", 0.1)
        
        # Experiment info
        experiment_id = config.get("experiment_id", "nasa_experiment")
        algorithm = config.get("algorithm", "fedavg")
        
        print("🚀 Starting NASA Federated Learning Server")
        print("=" * 50)
        print(f"Experiment ID: {experiment_id}")
        print(f"Algorithm: {algorithm.upper()}")
        print(f"Server: {server_host}:{server_port}")
        print(f"Rounds: {num_rounds}")
        print(f"Strategy: {strategy_name}")
        print(f"Min Clients: {min_available_clients}")
        print(f"Results Directory: {args.results_dir}")
        print("=" * 50)
        
        # Create results directory
        results_dir = os.path.join(args.results_dir, experiment_id)
        os.makedirs(results_dir, exist_ok=True)
        
        # Initialize metrics collector
        metrics_collector = MetricsCollector(results_dir, experiment_id)
        
        # Create strategy
        nasa_strategy = NASAStrategy(
            strategy_name=strategy_name,
            fraction_fit=fraction_fit,
            fraction_evaluate=fraction_evaluate,
            min_fit_clients=min_fit_clients,
            min_evaluate_clients=min_evaluate_clients,
            min_available_clients=min_available_clients,
            proximal_mu=proximal_mu
        )
        
        strategy = nasa_strategy.get_strategy()
        
        # Add metrics collection to the strategy
        class MetricsStrategy(type(strategy)):
            def aggregate_fit(self, server_round, results, failures):
                # Call parent method
                aggregated_parameters, aggregated_metrics = super().aggregate_fit(
                    server_round, results, failures
                )
                
                # Collect metrics
                metrics_collector.collect_fit_metrics(server_round, results, failures)
                
                # Add round number to config for clients
                config = {"current_round": server_round}
                
                return aggregated_parameters, aggregated_metrics
            
            def aggregate_evaluate(self, server_round, results, failures):
                # Call parent method
                aggregated_loss, aggregated_metrics = super().aggregate_evaluate(
                    server_round, results, failures
                )
                
                # For centralized evaluation, we'll handle it separately
                return aggregated_loss, aggregated_metrics
            
        

            def evaluate(self, server_round, parameters):
                # This is for centralized evaluation
                eval_metrics = {}
                
                if server_round > 0:
                    # In a real scenario, you would evaluate on a central test set here
                    # For now, we'll just log that evaluation occurred
                    eval_metrics = {
                        "rmse": 0.0,
                        "mse": 0.0, 
                        "mae": 0.0,
                        "r2": 0.0
                    }
                    metrics_collector.collect_evaluate_metrics(server_round, 0.0, eval_metrics)
                else:
                    # For round 0, return empty metrics
                    eval_metrics = {
                        "rmse": 0.0,
                        "mse": 0.0, 
                        "mae": 0.0,
                        "r2": 0.0
                    }
                
                return 0.0, eval_metrics
        
        # Create the enhanced strategy
        enhanced_strategy = MetricsStrategy(
            fraction_fit=fraction_fit,
            fraction_evaluate=fraction_evaluate,
            min_fit_clients=min_fit_clients,
            min_evaluate_clients=min_evaluate_clients,
            min_available_clients=min_available_clients
        )
        
        # Start Flower server
        print(f"🔄 Starting FL server on {server_host}:{server_port}...")
        
        fl.server.start_server(
            server_address=f"{server_host}:{server_port}",
            config=fl.server.ServerConfig(num_rounds=num_rounds),
            strategy=enhanced_strategy
        )
        
        print("✅ Server completed successfully!")
        print(f"📊 Results saved to: {results_dir}")
        
    except Exception as e:
        print(f"❌ Server error: {e}")
        raise

if __name__ == "__main__":
    main()