[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: train message f7c44298-ae22-44ea-bd56-0fdf1df3df07
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 80e634e4-5adf-498f-a0e4-9dbf28fc9d7b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 051d0e80-1b79-40cb-acd5-c749a3149587
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 38540f6a-68c9-4cbe-8c85-ce31c4081907
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 43ec328d-fea7-433a-a4cb-6661561f640b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 64974d75-72d3-4f31-a4c5-a5d82bd14ecc
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message dca90a5c-83b9-46ad-b766-11a53a668465
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 52f0ee9a-391d-41ab-9348-e91be1962b65
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 48431ec9-3988-45de-9f90-67bbd4871a9a
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message b8a27d00-ebcc-49c3-8479-1e910a8b7ebb
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 4814cada-630d-4de5-b083-e1519a0bd306
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 3feb17d4-a3c9-411b-9d99-564bc65ca8bd
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 942e10cf-9796-4eb2-91f7-86096fbd8cb4
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message c72b68ba-29e7-444d-9d55-8cba0032b304
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message e1014a19-9856-4c5e-a5df-abd3770dfeaa
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 3e5afee9-8327-4120-985b-b2d4ca4462e4
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 0aa9ce7b-b472-4b7e-bf10-39248b2b8f46
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 1679014f-5ed2-4c83-a492-3389f903bd1c
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message b8ff3983-af6b-405d-873d-7f10fd350670
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message a1707820-8643-49dc-bf0d-3e97510d591b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message f82d3842-d81a-43c7-9bb8-1f1016033561
[92mINFO [0m:      Disconnect and shut down
🚀 Starting NASA FL Client: client_18
Algorithm: FEDAVG
Server: localhost:8686
K-Folds: 5
Log Directory: logs
🔄 Using device: cuda
🎯 GPU: NVIDIA A100-SXM4-80GB
💾 Hyperparameters saved to: logs/client_18_hyperparams_20251029_150435.csv
🔍 Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_18
📊 Loaded 6975 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_18
🔄 Created sequences with length 10
   Final dataset shape: X (6965, 10, 24), y (6965,)
✅ Data split completed:
   Training samples: 4179
   Validation samples: 1393
   Test samples: 1393
   Model type: lstm
✅ Client client_18 ready:
   Model: LSTM
   Training: 4179 samples
   Device: cuda
   Validation: 1393 samples
   Test: 1393 samples
   Algorithm: fedavg
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs
⏩ Skipping CV for Round 1 (runs every 5 rounds)
Round 1: train_loss=7945.2954, val_loss=7342.9038, val_r2=-0.1884

🧪 Round 1 Evaluation Results:
   Test Loss: 9421.4219
   RMSE: 97.0640, MAE: 74.6196, R²: -0.5790
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv
⏩ Skipping CV for Round 2 (runs every 5 rounds)
Round 2: train_loss=6792.8345, val_loss=6269.0479, val_r2=-0.0146
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 2 Evaluation Results:
   Test Loss: 6002.0870
   RMSE: 77.4731, MAE: 61.6309, R²: -0.0060
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv
⏩ Skipping CV for Round 3 (runs every 5 rounds)

🎯 Round 3 Training Results:
   Training - Loss: 6738.3574, RMSE: 82.0875, R²: -0.0103
   Validation - Loss: 6223.5493, RMSE: 78.8895, R²: -0.0073

🧪 Round 3 Evaluation Results:
   Test Loss: 5991.9699
   RMSE: 77.4078, MAE: 61.6619, R²: -0.0043
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv
⏩ Skipping CV for Round 4 (runs every 5 rounds)
Round 4: train_loss=6741.4683, val_loss=6226.0674, val_r2=-0.0077
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 4 Evaluation Results:
   Test Loss: 5990.4809
   RMSE: 77.3982, MAE: 61.6676, R²: -0.0040
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv

🔍 Running 3-fold cross-validation (Round 5)

🔍 Starting 3-fold cross-validation on TRAINING data for client client_18

📊 Fold 1/3
   Fold 1 Results:
     Val Loss: 7020.9268
     Val RMSE: 83.7910, Val R²: -0.0095

📊 Fold 2/3
   Fold 2 Results:
     Val Loss: 6553.8750
     Val RMSE: 80.9560, Val R²: -0.0098

📊 Fold 3/3
   Fold 3 Results:
     Val Loss: 6631.6079
     Val RMSE: 81.4347, Val R²: -0.0108
💾 CV metrics saved to: logs/client_18_cv_metrics_20251029_150435.csv

📈 3-Fold CV Summary (Training Data):
   VAL_LOSS: 6735.4699 ± 204.3279
   RMSE: 82.0606 ± 1.2391
   R2: -0.0101 ± 0.0006

🎯 Round 5 Training Results:
   Training - Loss: 6743.7036, RMSE: 82.1201, R²: -0.0111
   Validation - Loss: 6227.8853, RMSE: 78.9170, R²: -0.0080
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 5 Evaluation Results:
   Test Loss: 5988.4079
   RMSE: 77.3848, MAE: 61.6771, R²: -0.0037
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv
⏩ Skipping CV for Round 6 (runs every 5 rounds)

🎯 Round 6 Training Results:
   Training - Loss: 6784.0200, RMSE: 82.3652, R²: -0.0172
   Validation - Loss: 6261.5371, RMSE: 79.1299, R²: -0.0134
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 6 Evaluation Results:
   Test Loss: 5987.6933
   RMSE: 77.3802, MAE: 61.6805, R²: -0.0035
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv
⏩ Skipping CV for Round 7 (runs every 5 rounds)
Round 7: train_loss=6746.5737, val_loss=6230.2295, val_r2=-0.0084

🧪 Round 7 Evaluation Results:
   Test Loss: 5987.8766
   RMSE: 77.3814, MAE: 61.6796, R²: -0.0036
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv
⏩ Skipping CV for Round 8 (runs every 5 rounds)
Round 8: train_loss=6792.2383, val_loss=6268.5527, val_r2=-0.0146
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 8 Evaluation Results:
   Test Loss: 5989.0562
   RMSE: 77.3890, MAE: 61.6741, R²: -0.0038
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv

🔍 Running 3-fold cross-validation (Round 9)
💾 CV metrics saved to: logs/client_18_cv_metrics_20251029_150435.csv

🎯 Round 9 Training Results:
   Training - Loss: 6742.3755, RMSE: 82.1120, R²: -0.0109
   Validation - Loss: 6226.8052, RMSE: 78.9101, R²: -0.0078
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 9 Evaluation Results:
   Test Loss: 5985.0807
   RMSE: 77.3633, MAE: 61.6934, R²: -0.0031
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv

🔍 Running 3-fold cross-validation (Round 10)
💾 CV metrics saved to: logs/client_18_cv_metrics_20251029_150435.csv

🎯 Round 10 Training Results:
   Training - Loss: 6744.2373, RMSE: 82.1233, R²: -0.0112
   Validation - Loss: 6228.3223, RMSE: 78.9197, R²: -0.0080
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_150435.csv

🧪 Round 10 Evaluation Results:
   Test Loss: 5985.7914
   RMSE: 77.3679, MAE: 61.6898, R²: -0.0032
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_150435.csv

================================================================================
🎯 FINAL COMPREHENSIVE REPORT
================================================================================
💾 Final summary saved to: logs/client_18_final_summary_20251029_150435.csv

📊 CLIENT: client_18 | ALGORITHM: fedavg | MODEL: LSTM
📈 TOTAL ROUNDS: 10

⚙️  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

🏁 FINAL ROUND PERFORMANCE:
   Training   - Loss:  6744.24 | RMSE:  82.12 | R²: -0.0112
   Validation - Loss:  6228.32 | RMSE:  78.92 | R²: -0.0080
   Test       - Loss:  5985.79 | RMSE:  77.37 | R²: -0.0032

📊 STATISTICS ACROSS ALL ROUNDS (Mean ± Std):
   Training Loss:    6762.98 ±  23.30
   Validation Loss:  6244.03 ±  19.50
   Test Loss:        6332.99 ± 1029.49

   Training RMSE:    82.24 ±  0.14
   Validation RMSE:  79.02 ±  0.12
   Test RMSE:        79.36 ±  5.90

   Training R²:     -0.0140 ± 0.0035
   Validation R²:   -0.0106 ± 0.0032
   Test R²:         -0.0614 ± 0.1725

⭐ BEST PERFORMANCE:
   Best Round: 9 (Test R²: -0.0031)

📋 DATA SUMMARY:
   Training samples:   4179
   Validation samples: 1393
   Test samples:       1393
   Total samples:      6965
================================================================================
✅ Client client_18 completed | Algorithm: FEDAVG
