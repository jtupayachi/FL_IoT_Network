[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 3e19a9dc-c187-4272-8ad8-0c5e87d9f0fa
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 9eae9fa7-8cd4-4652-87e7-12ce5797c013
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 5959c27d-7158-4f35-98ec-62cdb7d0226b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message cdb8fcc4-ad52-4dba-848c-a7edc004b849
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 68160de3-e0a5-409f-88ab-f1488ba2a026
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message fb85b6d7-d0ff-465d-9afa-df2b8808545b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 6c16bf40-d598-4941-ba26-62f209ceebff
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 0ea2cb0d-7ea0-4262-b49b-2b78d2a8c6aa
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 6b8201a7-8901-4db4-87c5-d6e5351a1250
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 35d92b51-8dd2-413d-a5e8-4ce293aca99f
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message eb20be3f-00cf-4230-ac8e-f9ab9d6c35d5
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 10492374-39d7-4e32-abee-4100cdddaf91
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 9da0025b-dd5c-4fbe-93d9-8a303044ab9d
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 7719aa6d-b70e-4055-8b9e-7f709c6b2518
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message ca8f2e63-0a98-45f6-bf1d-18b37029079e
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 74ca3746-dade-4a0a-a08d-9058221f1afb
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 3814a9e1-e7cd-4489-b25e-5380fc99e216
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 4be537c1-2be7-43ee-8936-b318ea5bb021
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message cd83fb38-a5f7-4f6b-9776-28ba6b197f7b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message 293dd141-cca9-491f-9ddd-e276e0c1c11c
[92mINFO [0m:      Disconnect and shut down
ğŸš€ Starting NASA FL Client: client_0
Algorithm: FEDAVG
Server: localhost:8686
K-Folds: 5
Log Directory: logs
ğŸ”„ Using device: cuda
ğŸ¯ GPU: NVIDIA A100-SXM4-80GB
ğŸ’¾ Hyperparameters saved to: logs/client_0_hyperparams_20251029_151810.csv
ğŸ” Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_0
ğŸ“Š Loaded 5015 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_0
ğŸ”„ Created sequences with length 10
   Final dataset shape: X (5005, 10, 24), y (5005,)
âœ… Data split completed:
   Training samples: 3003
   Validation samples: 1001
   Test samples: 1001
   Model type: lstm
âœ… Client client_0 ready:
   Model: LSTM
   Training: 3003 samples
   Device: cuda
   Validation: 1001 samples
   Test: 1001 samples
   Algorithm: fedavg
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs

ğŸ§ª Round 0 Evaluation Results:
   Test Loss: 8329.1998
   RMSE: 91.2645, MAE: 69.4190, RÂ²: -0.4969
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv
â© Skipping CV for Round 2 (runs every 5 rounds)
Round 2: train_loss=5932.1328, val_loss=6052.1094, val_r2=-0.0356
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 2 Evaluation Results:
   Test Loss: 5568.5814
   RMSE: 74.6229, MAE: 59.7885, RÂ²: -0.0008
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv
â© Skipping CV for Round 3 (runs every 5 rounds)

ğŸ¯ Round 3 Training Results:
   Training - Loss: 5858.6333, RMSE: 76.5417, RÂ²: -0.0197
   Validation - Loss: 5974.0488, RMSE: 77.2920, RÂ²: -0.0222

ğŸ§ª Round 3 Evaluation Results:
   Test Loss: 5585.4461
   RMSE: 74.7358, MAE: 60.0310, RÂ²: -0.0038
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv
â© Skipping CV for Round 4 (runs every 5 rounds)
Round 4: train_loss=5840.1211, val_loss=5954.1660, val_r2=-0.0188
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 4 Evaluation Results:
   Test Loss: 5584.7068
   RMSE: 74.7309, MAE: 60.0222, RÂ²: -0.0037
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv

ğŸ” Running 3-fold cross-validation (Round 5)

ğŸ” Starting 3-fold cross-validation on TRAINING data for client client_0

ğŸ“Š Fold 1/3
   Fold 1 Results:
     Val Loss: 6205.1929
     Val RMSE: 78.7730, Val RÂ²: -0.0386

ğŸ“Š Fold 2/3
   Fold 2 Results:
     Val Loss: 5673.7285
     Val RMSE: 75.3242, Val RÂ²: -0.0076

ğŸ“Š Fold 3/3
   Fold 3 Results:
     Val Loss: 5628.0269
     Val RMSE: 75.0202, Val RÂ²: -0.0056
ğŸ’¾ CV metrics saved to: logs/client_0_cv_metrics_20251029_151810.csv

ğŸ“ˆ 3-Fold CV Summary (Training Data):
   VAL_LOSS: 5835.6494 Â± 261.9719
   RMSE: 76.3725 Â± 1.7020
   R2: -0.0173 Â± 0.0151

ğŸ¯ Round 5 Training Results:
   Training - Loss: 5845.6855, RMSE: 76.4571, RÂ²: -0.0174
   Validation - Loss: 5960.1558, RMSE: 77.2020, RÂ²: -0.0198
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 5 Evaluation Results:
   Test Loss: 5591.0694
   RMSE: 74.7735, MAE: 60.0936, RÂ²: -0.0048
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv
â© Skipping CV for Round 6 (runs every 5 rounds)

ğŸ¯ Round 6 Training Results:
   Training - Loss: 5904.6758, RMSE: 76.8419, RÂ²: -0.0277
   Validation - Loss: 6023.0771, RMSE: 77.6085, RÂ²: -0.0306
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 6 Evaluation Results:
   Test Loss: 5590.1521
   RMSE: 74.7673, MAE: 60.0838, RÂ²: -0.0047
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv
â© Skipping CV for Round 7 (runs every 5 rounds)
Round 7: train_loss=5863.8574, val_loss=5979.6392, val_r2=-0.0232

ğŸ§ª Round 7 Evaluation Results:
   Test Loss: 5595.4262
   RMSE: 74.8026, MAE: 60.1387, RÂ²: -0.0056
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv
â© Skipping CV for Round 8 (runs every 5 rounds)
Round 8: train_loss=5842.8169, val_loss=5957.0688, val_r2=-0.0193
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 8 Evaluation Results:
   Test Loss: 5592.2166
   RMSE: 74.7811, MAE: 60.1055, RÂ²: -0.0050
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv

ğŸ” Running 3-fold cross-validation (Round 9)
ğŸ’¾ CV metrics saved to: logs/client_0_cv_metrics_20251029_151810.csv

ğŸ¯ Round 9 Training Results:
   Training - Loss: 5874.6387, RMSE: 76.6462, RÂ²: -0.0225
   Validation - Loss: 5991.1509, RMSE: 77.4025, RÂ²: -0.0251
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 9 Evaluation Results:
   Test Loss: 5586.4866
   RMSE: 74.7428, MAE: 60.0432, RÂ²: -0.0040
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv

ğŸ” Running 3-fold cross-validation (Round 10)
ğŸ’¾ CV metrics saved to: logs/client_0_cv_metrics_20251029_151810.csv

ğŸ¯ Round 10 Training Results:
   Training - Loss: 5875.6177, RMSE: 76.6526, RÂ²: -0.0226
   Validation - Loss: 5992.1948, RMSE: 77.4093, RÂ²: -0.0253
ğŸ’¾ Training metrics saved to: logs/client_0_training_metrics_20251029_151810.csv

ğŸ§ª Round 10 Evaluation Results:
   Test Loss: 5592.2463
   RMSE: 74.7813, MAE: 60.1058, RÂ²: -0.0050
ğŸ’¾ Test metrics saved to: logs/client_0_test_metrics_20251029_151810.csv

================================================================================
ğŸ¯ FINAL COMPREHENSIVE REPORT
================================================================================
ğŸ’¾ Final summary saved to: logs/client_0_final_summary_20251029_151810.csv

ğŸ“Š CLIENT: client_0 | ALGORITHM: fedavg | MODEL: LSTM
ğŸ“ˆ TOTAL ROUNDS: 10

âš™ï¸  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

ğŸ FINAL ROUND PERFORMANCE:
   Training   - Loss:  5875.62 | RMSE:  76.65 | RÂ²: -0.0226
   Validation - Loss:  5992.19 | RMSE:  77.41 | RÂ²: -0.0253
   Test       - Loss:  5592.25 | RMSE:  74.78 | RÂ²: -0.0050

ğŸ“Š STATISTICS ACROSS ALL ROUNDS (Mean Â± Std):
   Training Loss:    5873.67 Â±  32.17
   Validation Loss:  5989.99 Â±  34.26
   Test Loss:        5861.55 Â± 822.58

   Training RMSE:    76.64 Â±  0.21
   Validation RMSE:  77.39 Â±  0.22
   Test RMSE:        76.40 Â±  4.95

   Training RÂ²:     -0.0223 Â± 0.0056
   Validation RÂ²:   -0.0249 Â± 0.0059
   Test RÂ²:         -0.0534 Â± 0.1478

â­ BEST PERFORMANCE:
   Best Round: 2 (Test RÂ²: -0.0008)

ğŸ“‹ DATA SUMMARY:
   Training samples:   3003
   Validation samples: 1001
   Test samples:       1001
   Total samples:      5005
================================================================================
âœ… Client client_0 completed | Algorithm: FEDAVG
