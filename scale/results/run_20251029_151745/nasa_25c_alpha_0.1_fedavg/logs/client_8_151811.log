[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 61cac3f9-d1b9-4008-815a-80b78d5da6f2
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 86b67d0f-a831-4581-bd17-f1c1cfce37b4
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message ad0d799f-980a-478d-9792-2a4076a127ff
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message bf717178-3d0f-43b9-a1c1-6185d3b1edf1
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message c761462f-9090-4c72-b474-be6dd180abbf
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message fb508e69-ad6a-4b8f-aaff-297eadd9edfd
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 759cf602-1ebd-45d0-a62e-74116a56496c
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message a8386a0d-0d54-4c68-87a5-d440196e78a2
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message da48fcb3-e930-4f8d-8488-4fceae59dd45
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message ef45a0f4-a92d-42c8-8ccd-6319b5b8652a
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 19a68cef-f735-4f69-a51f-14e0d36f3b0b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 4e868526-0724-45b2-802e-08dee96864e7
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 64b93373-ea5c-4d3b-9cce-c5a86ec8dd7f
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 7e89b53c-c9a6-4f6f-9535-8c2fa4f32ce4
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 977f6280-4ce1-4700-9cca-9433f0e9a019
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message dce2e17e-ad1d-49e4-ab67-e1c006a98839
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message b44beba1-7c5b-417c-b349-44422826227c
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message e5fbb8ff-6556-495f-8caf-1c2c5ccaf418
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 011f1ae3-2330-423f-a617-1b319c9e2ce2
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message a5275f6e-149a-40bc-b707-e4d5af91299a
[92mINFO [0m:      Disconnect and shut down
ğŸš€ Starting NASA FL Client: client_8
Algorithm: FEDAVG
Server: localhost:8686
K-Folds: 5
Log Directory: logs
ğŸ”„ Using device: cuda
ğŸ¯ GPU: NVIDIA A100-SXM4-80GB
ğŸ’¾ Hyperparameters saved to: logs/client_8_hyperparams_20251029_151818.csv
ğŸ” Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_8
ğŸ“Š Loaded 5083 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_8
ğŸ”„ Created sequences with length 10
   Final dataset shape: X (5073, 10, 24), y (5073,)
âœ… Data split completed:
   Training samples: 3043
   Validation samples: 1015
   Test samples: 1015
   Model type: lstm
âœ… Client client_8 ready:
   Model: LSTM
   Training: 3043 samples
   Device: cuda
   Validation: 1015 samples
   Test: 1015 samples
   Algorithm: fedavg
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs

ğŸ§ª Round 0 Evaluation Results:
   Test Loss: 13192.0517
   RMSE: 114.8567, MAE: 87.0377, RÂ²: -0.6710
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv
â© Skipping CV for Round 2 (runs every 5 rounds)
Round 2: train_loss=8087.2246, val_loss=7993.6787, val_r2=-0.0161
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 2 Evaluation Results:
   Test Loss: 8222.7306
   RMSE: 90.6793, MAE: 72.1941, RÂ²: -0.0416
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv
â© Skipping CV for Round 3 (runs every 5 rounds)

ğŸ¯ Round 3 Training Results:
   Training - Loss: 8074.8330, RMSE: 89.8601, RÂ²: -0.0169
   Validation - Loss: 7982.1567, RMSE: 89.3429, RÂ²: -0.0147

ğŸ§ª Round 3 Evaluation Results:
   Test Loss: 8137.8262
   RMSE: 90.2099, MAE: 72.0896, RÂ²: -0.0308
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv
â© Skipping CV for Round 4 (runs every 5 rounds)
Round 4: train_loss=8089.6953, val_loss=7995.9810, val_r2=-0.0164
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 4 Evaluation Results:
   Test Loss: 8140.3583
   RMSE: 90.2239, MAE: 72.0925, RÂ²: -0.0311
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv

ğŸ” Running 3-fold cross-validation (Round 5)

ğŸ” Starting 3-fold cross-validation on TRAINING data for client client_8

ğŸ“Š Fold 1/3
   Fold 1 Results:
     Val Loss: 8529.8115
     Val RMSE: 92.3570, Val RÂ²: -0.0224

ğŸ“Š Fold 2/3
   Fold 2 Results:
     Val Loss: 8168.7725
     Val RMSE: 90.3813, Val RÂ²: -0.0292

ğŸ“Š Fold 3/3
   Fold 3 Results:
     Val Loss: 7564.7666
     Val RMSE: 86.9757, Val RÂ²: -0.0059
ğŸ’¾ CV metrics saved to: logs/client_8_cv_metrics_20251029_151818.csv

ğŸ“ˆ 3-Fold CV Summary (Training Data):
   VAL_LOSS: 8087.7835 Â± 398.1183
   RMSE: 89.9046 Â± 2.2226
   R2: -0.0192 Â± 0.0098

ğŸ¯ Round 5 Training Results:
   Training - Loss: 8050.0103, RMSE: 89.7218, RÂ²: -0.0137
   Validation - Loss: 7959.2080, RMSE: 89.2144, RÂ²: -0.0117
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 5 Evaluation Results:
   Test Loss: 8120.2412
   RMSE: 90.1124, MAE: 72.0686, RÂ²: -0.0286
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv
â© Skipping CV for Round 6 (runs every 5 rounds)

ğŸ¯ Round 6 Training Results:
   Training - Loss: 8049.7603, RMSE: 89.7205, RÂ²: -0.0137
   Validation - Loss: 7958.9775, RMSE: 89.2131, RÂ²: -0.0117
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 6 Evaluation Results:
   Test Loss: 8122.9314
   RMSE: 90.1273, MAE: 72.0719, RÂ²: -0.0289
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv
â© Skipping CV for Round 7 (runs every 5 rounds)
Round 7: train_loss=7987.8857, val_loss=7903.0498, val_r2=-0.0046

ğŸ§ª Round 7 Evaluation Results:
   Test Loss: 8108.2510
   RMSE: 90.0458, MAE: 72.0549, RÂ²: -0.0271
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv
â© Skipping CV for Round 8 (runs every 5 rounds)
Round 8: train_loss=8082.0493, val_loss=7988.8633, val_r2=-0.0155
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 8 Evaluation Results:
   Test Loss: 8116.9621
   RMSE: 90.0942, MAE: 72.0646, RÂ²: -0.0282
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv

ğŸ” Running 3-fold cross-validation (Round 9)
ğŸ’¾ CV metrics saved to: logs/client_8_cv_metrics_20251029_151818.csv

ğŸ¯ Round 9 Training Results:
   Training - Loss: 8034.3604, RMSE: 89.6346, RÂ²: -0.0118
   Validation - Loss: 7944.8501, RMSE: 89.1339, RÂ²: -0.0099
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 9 Evaluation Results:
   Test Loss: 8134.3597
   RMSE: 90.1907, MAE: 72.0855, RÂ²: -0.0304
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv

ğŸ” Running 3-fold cross-validation (Round 10)
ğŸ’¾ CV metrics saved to: logs/client_8_cv_metrics_20251029_151818.csv

ğŸ¯ Round 10 Training Results:
   Training - Loss: 8062.5269, RMSE: 89.7916, RÂ²: -0.0153
   Validation - Loss: 7970.7549, RMSE: 89.2791, RÂ²: -0.0132
ğŸ’¾ Training metrics saved to: logs/client_8_training_metrics_20251029_151818.csv

ğŸ§ª Round 10 Evaluation Results:
   Test Loss: 8116.8797
   RMSE: 90.0937, MAE: 72.0645, RÂ²: -0.0281
ğŸ’¾ Test metrics saved to: logs/client_8_test_metrics_20251029_151818.csv

================================================================================
ğŸ¯ FINAL COMPREHENSIVE REPORT
================================================================================
ğŸ’¾ Final summary saved to: logs/client_8_final_summary_20251029_151818.csv

ğŸ“Š CLIENT: client_8 | ALGORITHM: fedavg | MODEL: LSTM
ğŸ“ˆ TOTAL ROUNDS: 10

âš™ï¸  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

ğŸ FINAL ROUND PERFORMANCE:
   Training   - Loss:  8062.53 | RMSE:  89.79 | RÂ²: -0.0153
   Validation - Loss:  7970.75 | RMSE:  89.28 | RÂ²: -0.0132
   Test       - Loss:  8116.88 | RMSE:  90.09 | RÂ²: -0.0281

ğŸ“Š STATISTICS ACROSS ALL ROUNDS (Mean Â± Std):
   Training Loss:    8065.09 Â±  19.99
   Validation Loss:  7973.19 Â±  18.48
   Test Loss:        8641.26 Â± 1517.24

   Training RMSE:    89.81 Â±  0.11
   Validation RMSE:  89.29 Â±  0.10
   Test RMSE:        92.66 Â±  7.40

   Training RÂ²:     -0.0156 Â± 0.0025
   Validation RÂ²:   -0.0135 Â± 0.0023
   Test RÂ²:         -0.0946 Â± 0.1922

â­ BEST PERFORMANCE:
   Best Round: 7 (Test RÂ²: -0.0271)

ğŸ“‹ DATA SUMMARY:
   Training samples:   3043
   Validation samples: 1015
   Test samples:       1015
   Total samples:      5073
================================================================================
âœ… Client client_8 completed | Algorithm: FEDAVG
