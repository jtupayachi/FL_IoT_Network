[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 34112532-fcc7-4ffe-880f-0a881cb0459a
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 7f101c4a-295d-4b23-83e2-c9f73b3d3959
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 877201e9-0cb9-4d91-87e2-f62e92b3201c
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 20a77a8b-7bad-4355-89b4-85e1eef5ec85
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 0a86a5b0-b308-4140-b2b5-6d0f63d0c710
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 7d4cc9e7-0ba7-46b8-af1f-0a1b38ce562b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 210be4f4-9823-404b-9e8a-34cc477ce598
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 76f15817-f4b9-4783-ba3f-2ad54394081e
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 2485d55a-8ca2-410c-9708-5bccba9129d7
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message e5748912-178e-427b-8995-0d848a9ab6a1
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 9b08a00c-196f-49fd-8177-c9d42d1db82d
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 1811f274-901d-4f7f-99a6-eee21b6fffa7
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message a32fa51e-5e4b-4250-b874-2aeb64da13c9
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message e105f213-e19d-49f4-8c3e-790d90e30341
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 7f7790ee-0372-4444-86c8-2251cec0bdff
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 6cee3436-688a-4d17-9df3-4b71f53ffe30
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message a5a4557c-d5cf-4a65-8941-1e5d2964da44
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 3961f715-8a19-43c2-aaee-a0402f5d08e2
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 52fed586-6177-4de6-b863-1f68a6312e5f
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message a7620891-4ed9-4fd1-b19f-560444e953bf
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message ab33046e-5de3-4784-a4c7-7f2cdd9e59e1
[92mINFO [0m:      Disconnect and shut down
🚀 Starting NASA FL Client: client_15
Algorithm: FEDAVG
Server: localhost:8686
K-Folds: 5
Log Directory: logs
🔄 Using device: cuda
🎯 GPU: NVIDIA A100-SXM4-80GB
💾 Hyperparameters saved to: logs/client_15_hyperparams_20251029_152232.csv
🔍 Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_15
📊 Loaded 6769 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_15
🔄 Created sequences with length 10
   Final dataset shape: X (6759, 10, 24), y (6759,)
✅ Data split completed:
   Training samples: 4055
   Validation samples: 1352
   Test samples: 1352
   Model type: lstm
✅ Client client_15 ready:
   Model: LSTM
   Training: 4055 samples
   Device: cuda
   Validation: 1352 samples
   Test: 1352 samples
   Algorithm: fedavg
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs
⏩ Skipping CV for Round 1 (runs every 5 rounds)
Round 1: train_loss=7662.3696, val_loss=7470.4780, val_r2=-0.0838

🧪 Round 1 Evaluation Results:
   Test Loss: 9177.0864
   RMSE: 95.7971, MAE: 72.3428, R²: -0.2216
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv
⏩ Skipping CV for Round 2 (runs every 5 rounds)
Round 2: train_loss=7186.9370, val_loss=7006.7969, val_r2=-0.0165
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 2 Evaluation Results:
   Test Loss: 7532.4638
   RMSE: 86.7898, MAE: 69.5074, R²: -0.0027
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv
⏩ Skipping CV for Round 3 (runs every 5 rounds)

🎯 Round 3 Training Results:
   Training - Loss: 7140.3359, RMSE: 84.5005, R²: -0.0109
   Validation - Loss: 6962.2568, RMSE: 83.4401, R²: -0.0101

🧪 Round 3 Evaluation Results:
   Test Loss: 7525.1282
   RMSE: 86.7475, MAE: 69.5939, R²: -0.0017
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv
⏩ Skipping CV for Round 4 (runs every 5 rounds)
Round 4: train_loss=7120.7764, val_loss=6943.7500, val_r2=-0.0074
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 4 Evaluation Results:
   Test Loss: 7526.1787
   RMSE: 86.7536, MAE: 69.5798, R²: -0.0018
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv

🔍 Running 3-fold cross-validation (Round 5)

🔍 Starting 3-fold cross-validation on TRAINING data for client client_15

📊 Fold 1/3
   Fold 1 Results:
     Val Loss: 6745.0889
     Val RMSE: 82.1285, Val R²: -0.0018

📊 Fold 2/3
   Fold 2 Results:
     Val Loss: 7678.4453
     Val RMSE: 87.6267, Val R²: -0.0347

📊 Fold 3/3
   Fold 3 Results:
     Val Loss: 7187.8208
     Val RMSE: 84.7810, Val R²: -0.0295
💾 CV metrics saved to: logs/client_15_cv_metrics_20251029_152232.csv

📈 3-Fold CV Summary (Training Data):
   VAL_LOSS: 7203.7850 ± 381.2083
   RMSE: 84.8454 ± 2.2451
   R2: -0.0220 ± 0.0145

🎯 Round 5 Training Results:
   Training - Loss: 7169.4648, RMSE: 84.6727, R²: -0.0150
   Validation - Loss: 6990.0430, RMSE: 83.6065, R²: -0.0141
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 5 Evaluation Results:
   Test Loss: 7520.8125
   RMSE: 86.7226, MAE: 69.6612, R²: -0.0011
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv
⏩ Skipping CV for Round 6 (runs every 5 rounds)

🎯 Round 6 Training Results:
   Training - Loss: 7135.3848, RMSE: 84.4712, R²: -0.0102
   Validation - Loss: 6957.5586, RMSE: 83.4120, R²: -0.0094
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 6 Evaluation Results:
   Test Loss: 7521.6059
   RMSE: 86.7272, MAE: 69.6472, R²: -0.0012
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv
⏩ Skipping CV for Round 7 (runs every 5 rounds)
Round 7: train_loss=7126.1240, val_loss=6948.7939, val_r2=-0.0081

🧪 Round 7 Evaluation Results:
   Test Loss: 7522.1644
   RMSE: 86.7304, MAE: 69.6377, R²: -0.0013
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv
⏩ Skipping CV for Round 8 (runs every 5 rounds)
Round 8: train_loss=7199.8921, val_loss=7019.2515, val_r2=-0.0183
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 8 Evaluation Results:
   Test Loss: 7524.6234
   RMSE: 86.7446, MAE: 69.6009, R²: -0.0016
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv

🔍 Running 3-fold cross-validation (Round 9)
💾 CV metrics saved to: logs/client_15_cv_metrics_20251029_152232.csv

🎯 Round 9 Training Results:
   Training - Loss: 7165.1108, RMSE: 84.6470, R²: -0.0144
   Validation - Loss: 6985.8760, RMSE: 83.5816, R²: -0.0135
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 9 Evaluation Results:
   Test Loss: 7523.0155
   RMSE: 86.7353, MAE: 69.6240, R²: -0.0014
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv

🔍 Running 3-fold cross-validation (Round 10)
💾 CV metrics saved to: logs/client_15_cv_metrics_20251029_152232.csv

🎯 Round 10 Training Results:
   Training - Loss: 7155.9780, RMSE: 84.5930, R²: -0.0131
   Validation - Loss: 6977.1509, RMSE: 83.5293, R²: -0.0122
💾 Training metrics saved to: logs/client_15_training_metrics_20251029_152232.csv

🧪 Round 10 Evaluation Results:
   Test Loss: 7522.0358
   RMSE: 86.7297, MAE: 69.6399, R²: -0.0013
💾 Test metrics saved to: logs/client_15_test_metrics_20251029_152232.csv

================================================================================
🎯 FINAL COMPREHENSIVE REPORT
================================================================================
💾 Final summary saved to: logs/client_15_final_summary_20251029_152232.csv

📊 CLIENT: client_15 | ALGORITHM: fedavg | MODEL: LSTM
📈 TOTAL ROUNDS: 10

⚙️  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

🏁 FINAL ROUND PERFORMANCE:
   Training   - Loss:  7155.98 | RMSE:  84.59 | R²: -0.0131
   Validation - Loss:  6977.15 | RMSE:  83.53 | R²: -0.0122
   Test       - Loss:  7522.04 | RMSE:  86.73 | R²: -0.0013

📊 STATISTICS ACROSS ALL ROUNDS (Mean ± Std):
   Training Loss:    7161.93 ±  25.54
   Validation Loss:  6982.92 ±  24.38
   Test Loss:        7689.51 ± 495.87

   Training RMSE:    84.63 ±  0.15
   Validation RMSE:  83.56 ±  0.15
   Test RMSE:        87.65 ±  2.72

   Training R²:     -0.0139 ± 0.0036
   Validation R²:   -0.0131 ± 0.0035
   Test R²:         -0.0236 ± 0.0660

⭐ BEST PERFORMANCE:
   Best Round: 5 (Test R²: -0.0011)

📋 DATA SUMMARY:
   Training samples:   4055
   Validation samples: 1352
   Test samples:       1352
   Total samples:      6759
================================================================================
✅ Client client_15 completed | Algorithm: FEDAVG
