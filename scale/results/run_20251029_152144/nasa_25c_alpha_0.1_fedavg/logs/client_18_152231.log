[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 9c242905-9616-4da1-ad9c-e970e241976a
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 6e7420db-f586-45a8-aedd-739d46e998ee
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message fc00ab3d-89e8-420b-a9f5-538a05992b1d
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 6494335c-1322-4001-81a4-afd765ed7905
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message fdc04828-ec01-48fe-a99c-60e7dc3c2826
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message c662a354-c9bd-42b9-9ce3-f5421eeb77c2
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message e615ba6d-1524-4ca3-b5d0-38c227f14e30
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 0e28a678-7734-4808-92d2-0b3a375ba3eb
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 1e17f9b3-747a-4c4e-82d4-395c0782d8eb
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message e62eef27-4cbd-435c-8924-5e034895ad4c
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message b90e5851-d97a-41be-aeea-a70a6a1cec2d
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 225ee2d8-a1c1-488b-a4a3-5ab970b0e9d6
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 115da7f9-274c-4f7b-809f-c93e777a6630
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 558f7ab0-1b2c-4e37-b098-693aa5ed7635
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 9da1c332-f12d-4993-bccb-1a6cc2078995
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 4531fc58-4c73-4415-9685-878fbfbea68b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 4690aa78-d178-47fa-8496-9fe3b06942bd
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 4aa577b3-b2af-4bbf-9d48-4e4e7a689394
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 5b20089f-e846-4be0-b9ae-adcbb0663e1e
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 5f3fbea9-4241-4c8f-8839-df3417171c18
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message 4545a08a-5f9d-4030-a6b6-d045c4533c17
[92mINFO [0m:      Disconnect and shut down
🚀 Starting NASA FL Client: client_18
Algorithm: FEDAVG
Server: localhost:8686
K-Folds: 5
Log Directory: logs
🔄 Using device: cuda
🎯 GPU: NVIDIA A100-SXM4-80GB
💾 Hyperparameters saved to: logs/client_18_hyperparams_20251029_152240.csv
🔍 Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_18
📊 Loaded 6975 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.1/client_18
🔄 Created sequences with length 10
   Final dataset shape: X (6965, 10, 24), y (6965,)
✅ Data split completed:
   Training samples: 4179
   Validation samples: 1393
   Test samples: 1393
   Model type: lstm
✅ Client client_18 ready:
   Model: LSTM
   Training: 4179 samples
   Device: cuda
   Validation: 1393 samples
   Test: 1393 samples
   Algorithm: fedavg
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs
⏩ Skipping CV for Round 1 (runs every 5 rounds)
Round 1: train_loss=6946.3423, val_loss=6404.9097, val_r2=-0.0366

🧪 Round 1 Evaluation Results:
   Test Loss: 7609.5522
   RMSE: 87.2327, MAE: 66.9548, R²: -0.2754
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv
⏩ Skipping CV for Round 2 (runs every 5 rounds)
Round 2: train_loss=6783.9707, val_loss=6261.5054, val_r2=-0.0134
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 2 Evaluation Results:
   Test Loss: 5984.3445
   RMSE: 77.3585, MAE: 61.6972, R²: -0.0030
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv
⏩ Skipping CV for Round 3 (runs every 5 rounds)

🎯 Round 3 Training Results:
   Training - Loss: 6773.8564, RMSE: 82.3034, R²: -0.0157
   Validation - Loss: 6252.9346, RMSE: 79.0755, R²: -0.0120

🧪 Round 3 Evaluation Results:
   Test Loss: 5977.4953
   RMSE: 77.3143, MAE: 61.7399, R²: -0.0018
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv
⏩ Skipping CV for Round 4 (runs every 5 rounds)
Round 4: train_loss=6814.7280, val_loss=6287.9453, val_r2=-0.0177
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 4 Evaluation Results:
   Test Loss: 5978.4688
   RMSE: 77.3206, MAE: 61.7329, R²: -0.0020
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv

🔍 Running 3-fold cross-validation (Round 5)

🔍 Starting 3-fold cross-validation on TRAINING data for client client_18

📊 Fold 1/3
   Fold 1 Results:
     Val Loss: 6962.3545
     Val RMSE: 83.4407, Val R²: -0.0011

📊 Fold 2/3
   Fold 2 Results:
     Val Loss: 6543.9585
     Val RMSE: 80.8947, Val R²: -0.0083

📊 Fold 3/3
   Fold 3 Results:
     Val Loss: 6606.8184
     Val RMSE: 81.2823, Val R²: -0.0071
💾 CV metrics saved to: logs/client_18_cv_metrics_20251029_152240.csv

📈 3-Fold CV Summary (Training Data):
   VAL_LOSS: 6704.3771 ± 184.2138
   RMSE: 81.8726 ± 1.1201
   R2: -0.0055 ± 0.0031

🎯 Round 5 Training Results:
   Training - Loss: 6721.4150, RMSE: 81.9842, R²: -0.0078
   Validation - Loss: 6210.1040, RMSE: 78.8042, R²: -0.0051
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 5 Evaluation Results:
   Test Loss: 5973.5351
   RMSE: 77.2886, MAE: 61.7741, R²: -0.0012
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv
⏩ Skipping CV for Round 6 (runs every 5 rounds)

🎯 Round 6 Training Results:
   Training - Loss: 6724.7690, RMSE: 82.0047, R²: -0.0083
   Validation - Loss: 6212.7271, RMSE: 78.8209, R²: -0.0055
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 6 Evaluation Results:
   Test Loss: 5974.2571
   RMSE: 77.2933, MAE: 61.7668, R²: -0.0013
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv
⏩ Skipping CV for Round 7 (runs every 5 rounds)
Round 7: train_loss=6754.4995, val_loss=6236.7588, val_r2=-0.0094

🧪 Round 7 Evaluation Results:
   Test Loss: 5974.7675
   RMSE: 77.2966, MAE: 61.7618, R²: -0.0014
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv
⏩ Skipping CV for Round 8 (runs every 5 rounds)
Round 8: train_loss=6729.2759, val_loss=6216.2856, val_r2=-0.0061
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 8 Evaluation Results:
   Test Loss: 5977.0285
   RMSE: 77.3112, MAE: 61.7433, R²: -0.0018
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv

🔍 Running 3-fold cross-validation (Round 9)
💾 CV metrics saved to: logs/client_18_cv_metrics_20251029_152240.csv

🎯 Round 9 Training Results:
   Training - Loss: 6699.0962, RMSE: 81.8480, R²: -0.0044
   Validation - Loss: 6193.4180, RMSE: 78.6983, R²: -0.0024
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 9 Evaluation Results:
   Test Loss: 5975.5476
   RMSE: 77.3017, MAE: 61.7547, R²: -0.0015
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv

🔍 Running 3-fold cross-validation (Round 10)
💾 CV metrics saved to: logs/client_18_cv_metrics_20251029_152240.csv

🎯 Round 10 Training Results:
   Training - Loss: 6743.7939, RMSE: 82.1206, R²: -0.0111
   Validation - Loss: 6227.9692, RMSE: 78.9175, R²: -0.0080
💾 Training metrics saved to: logs/client_18_training_metrics_20251029_152240.csv

🧪 Round 10 Evaluation Results:
   Test Loss: 5974.6497
   RMSE: 77.2959, MAE: 61.7629, R²: -0.0014
💾 Test metrics saved to: logs/client_18_test_metrics_20251029_152240.csv

================================================================================
🎯 FINAL COMPREHENSIVE REPORT
================================================================================
💾 Final summary saved to: logs/client_18_final_summary_20251029_152240.csv

📊 CLIENT: client_18 | ALGORITHM: fedavg | MODEL: LSTM
📈 TOTAL ROUNDS: 10

⚙️  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

🏁 FINAL ROUND PERFORMANCE:
   Training   - Loss:  6743.79 | RMSE:  82.12 | R²: -0.0111
   Validation - Loss:  6227.97 | RMSE:  78.92 | R²: -0.0080
   Test       - Loss:  5974.65 | RMSE:  77.30 | R²: -0.0014

📊 STATISTICS ACROSS ALL ROUNDS (Mean ± Std):
   Training Loss:    6745.29 ±  37.23
   Validation Loss:  6229.99 ±  30.65
   Test Loss:        6139.96 ± 489.87

   Training RMSE:    82.13 ±  0.23
   Validation RMSE:  78.93 ±  0.19
   Test RMSE:        78.30 ±  2.98

   Training R²:     -0.0114 ± 0.0056
   Validation R²:   -0.0083 ± 0.0050
   Test R²:         -0.0291 ± 0.0821

⭐ BEST PERFORMANCE:
   Best Round: 5 (Test R²: -0.0012)

📋 DATA SUMMARY:
   Training samples:   4179
   Validation samples: 1393
   Test samples:       1393
   Total samples:      6965
================================================================================
✅ Client client_18 completed | Algorithm: FEDAVG
