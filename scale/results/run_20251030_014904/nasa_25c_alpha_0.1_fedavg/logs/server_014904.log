[93mWARNING [0m:   DEPRECATED FEATURE: flwr.server.start_server() is deprecated.
	Instead, use the `flower-superlink` CLI command to start a SuperLink as shown below:

		$ flower-superlink --insecure

	To view usage and all available options, run:

		$ flower-superlink --help

	Using `start_server()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower server, config: num_rounds=10, no round_timeout
[92mINFO [0m:      Flower ECE: gRPC server running (10 rounds), SSL is disabled
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      initial parameters (loss, other metrics): 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 12 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 12 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (1, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 26.003600605999964)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (2, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 28.6339594330002)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (3, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 31.28184253800009)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (4, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 33.79517283599944)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (5, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 38.76128626599984)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (6, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 41.29315660900011)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (7, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 43.797788115)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (8, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 46.33070686700012)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (9, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 51.34246054799951)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_fit: received 25 results and 0 failures
Traceback (most recent call last):
  File "/mnt/ceph_drive/FL_IoT_Network/scale/server.py", line 939, in evaluate_fn
    model.load_state_dict(state_dict, strict=True)
  File "/mnt/ceph_drive/FL_IoT_Network/flwr-nasa/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2629, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
[92mINFO [0m:      fit progress: (10, 10.0, {'rmse': 10.0, 'mse': 100.0, 'mae': 10.0, 'r2': -1.0}, 56.27819683500002)
[92mINFO [0m:      configure_evaluate: strategy sampled 25 clients (out of 25)
[92mINFO [0m:      aggregate_evaluate: received 25 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 10 round(s) in 56.61s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 16394.610455106165
[92mINFO [0m:      		round 2: 7527.426577247686
[92mINFO [0m:      		round 3: 7494.787035115017
[92mINFO [0m:      		round 4: 7491.455087390241
[92mINFO [0m:      		round 5: 7453.666195077678
[92mINFO [0m:      		round 6: 7464.187405882642
[92mINFO [0m:      		round 7: 7435.884009794587
[92mINFO [0m:      		round 8: 7441.840408507835
[92mINFO [0m:      		round 9: 7448.747149964424
[92mINFO [0m:      		round 10: 7441.538973053613
[92mINFO [0m:      	History (loss, centralized):
[92mINFO [0m:      		round 0: 10.0
[92mINFO [0m:      		round 1: 10.0
[92mINFO [0m:      		round 2: 10.0
[92mINFO [0m:      		round 3: 10.0
[92mINFO [0m:      		round 4: 10.0
[92mINFO [0m:      		round 5: 10.0
[92mINFO [0m:      		round 6: 10.0
[92mINFO [0m:      		round 7: 10.0
[92mINFO [0m:      		round 8: 10.0
[92mINFO [0m:      		round 9: 10.0
[92mINFO [0m:      		round 10: 10.0
[92mINFO [0m:      	History (metrics, centralized):
[92mINFO [0m:      	{'mae': [(0, 10.0),
[92mINFO [0m:      	         (1, 10.0),
[92mINFO [0m:      	         (2, 10.0),
[92mINFO [0m:      	         (3, 10.0),
[92mINFO [0m:      	         (4, 10.0),
[92mINFO [0m:      	         (5, 10.0),
[92mINFO [0m:      	         (6, 10.0),
[92mINFO [0m:      	         (7, 10.0),
[92mINFO [0m:      	         (8, 10.0),
[92mINFO [0m:      	         (9, 10.0),
[92mINFO [0m:      	         (10, 10.0)],
[92mINFO [0m:      	 'mse': [(0, 100.0),
[92mINFO [0m:      	         (1, 100.0),
[92mINFO [0m:      	         (2, 100.0),
[92mINFO [0m:      	         (3, 100.0),
[92mINFO [0m:      	         (4, 100.0),
[92mINFO [0m:      	         (5, 100.0),
[92mINFO [0m:      	         (6, 100.0),
[92mINFO [0m:      	         (7, 100.0),
[92mINFO [0m:      	         (8, 100.0),
[92mINFO [0m:      	         (9, 100.0),
[92mINFO [0m:      	         (10, 100.0)],
[92mINFO [0m:      	 'r2': [(0, -1.0),
[92mINFO [0m:      	        (1, -1.0),
[92mINFO [0m:      	        (2, -1.0),
[92mINFO [0m:      	        (3, -1.0),
[92mINFO [0m:      	        (4, -1.0),
[92mINFO [0m:      	        (5, -1.0),
[92mINFO [0m:      	        (6, -1.0),
[92mINFO [0m:      	        (7, -1.0),
[92mINFO [0m:      	        (8, -1.0),
[92mINFO [0m:      	        (9, -1.0),
[92mINFO [0m:      	        (10, -1.0)],
[92mINFO [0m:      	 'rmse': [(0, 10.0),
[92mINFO [0m:      	          (1, 10.0),
[92mINFO [0m:      	          (2, 10.0),
[92mINFO [0m:      	          (3, 10.0),
[92mINFO [0m:      	          (4, 10.0),
[92mINFO [0m:      	          (5, 10.0),
[92mINFO [0m:      	          (6, 10.0),
[92mINFO [0m:      	          (7, 10.0),
[92mINFO [0m:      	          (8, 10.0),
[92mINFO [0m:      	          (9, 10.0),
[92mINFO [0m:      	          (10, 10.0)]}
[92mINFO [0m:      
🚀 Starting NASA Federated Learning Server
==================================================
Experiment ID: nasa_25c_alpha_0.1_fedavg
Algorithm: FEDAVG
Server: localhost:8686
Rounds: 10
📁 Directory structure:
   Results root: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg
   Experiment: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg
   Metrics: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg/metrics
==================================================
🔍 Server model configuration:
   Input dimension: 5
   Model type: lstm
   Hidden dims: [64, 32]
🔄 Starting FL server on localhost:8686...
📊 Centralized evaluation enabled: True
📈 CSV files will be saved to: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg/metrics
📁 Config saved to: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg/config.json
🔍 Running centralized evaluation for round 0...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 0 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
⚠️ Could not update round metrics with centralized results: list index out of range
📊 Round 1 Fit Metrics:
   Clients: 12
   Avg Train Loss: 14879.9421
   Avg Val Loss: 14928.3574
   Avg Val R²: -1.2566
🔍 Running centralized evaluation for round 1...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 1 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 2 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7259.4792
   Avg Val Loss: 7280.4764
   Avg Val R²: -0.0296
🔍 Running centralized evaluation for round 2...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 2 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 3 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7231.9570
   Avg Val Loss: 7251.5702
   Avg Val R²: -0.0257
🔍 Running centralized evaluation for round 3...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 3 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 4 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7221.0371
   Avg Val Loss: 7240.2168
   Avg Val R²: -0.0239
🔍 Running centralized evaluation for round 4...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 4 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 5 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7193.3383
   Avg Val Loss: 7211.6694
   Avg Val R²: -0.0198
🔍 Running centralized evaluation for round 5...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 5 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 6 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7198.9540
   Avg Val Loss: 7220.1854
   Avg Val R²: -0.0212
🔍 Running centralized evaluation for round 6...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 6 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 7 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7179.3029
   Avg Val Loss: 7193.5356
   Avg Val R²: -0.0171
🔍 Running centralized evaluation for round 7...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 7 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 8 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7184.0295
   Avg Val Loss: 7202.9528
   Avg Val R²: -0.0187
🔍 Running centralized evaluation for round 8...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 8 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 9 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7175.5942
   Avg Val Loss: 7194.7161
   Avg Val R²: -0.0176
🔍 Running centralized evaluation for round 9...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 9 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
📊 Round 10 Fit Metrics:
   Clients: 25
   Avg Train Loss: 7178.3182
   Avg Val Loss: 7193.6923
   Avg Val R²: -0.0173
🔍 Running centralized evaluation for round 10...
📐 Detected from parameters:
   Input dim: 5 (expected: 5)
   First hidden: 256
   Hidden architecture: [256, 256, 32]
⚠️ Centralized evaluation error: Error(s) in loading state_dict for LSTMModel:
	size mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([256, 5]) from checkpoint, the shape in current model is torch.Size([1024, 5]).
	size mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([1024, 256]).
	size mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for fc_layers.0.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([32, 256]).
📈 Round 10 Centralized Test Evaluation:
   Loss: 10.0000
   RMSE: 10.0000
   R²: -1.0000
✅ Server completed successfully!
📊 Results saved to: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg

📈 FINAL SUMMARY:
   Round metrics: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg/metrics/round_metrics.csv
   Client metrics: /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg/metrics/client_metrics.csv
   Test metrics (centralized): /mnt/ceph_drive/FL_IoT_Network/scale/results/run_20251030_014904/nasa_25c_alpha_0.1_fedavg/nasa_25c_alpha_0.1_fedavg/metrics/test_metrics.csv
✅ round_metrics.csv: 10 records
✅ client_metrics.csv: 237 records
✅ test_metrics.csv: 11 records
