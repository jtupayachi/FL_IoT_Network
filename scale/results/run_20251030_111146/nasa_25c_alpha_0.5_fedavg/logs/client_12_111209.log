[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 9a335afe-7216-4171-ae67-2b5b3e5e5459
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 3c7eb20a-75a2-49f3-9bcf-5c3bf6c27d6d
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message a1b91ac4-3381-4cf4-a30e-3a056819f88d
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message c4566e40-4c16-4b44-ad32-667e2811d86b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 9001e950-25b3-4f51-9b3f-14878d59a3d7
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message d69ae6be-ce95-4835-866a-a17c094ac541
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 64949468-5fcd-4626-b1d9-88ffa0629c69
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 41432d17-bde7-463f-b6cb-e06952f1da80
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message c6b3d9f6-7ed5-4788-8902-25e4ac81193f
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 9c9dd9e4-f690-428a-b1aa-d494ae0bbaa0
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 0adc05e2-9bb1-4bd3-87b8-42c3eb4c015a
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 19af04cd-fde4-4285-8740-28d49301fe91
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 8f9deddf-3a3b-47c3-bc62-25bbc23d44ad
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 0aee1ac9-887e-473b-9c3b-e8fc17ead463
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message e63d5e11-edb7-4640-aabc-365b7a14f279
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message ae3aedf5-e6f4-4a4d-bdfe-605a03c0b36c
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 95494cbd-f4ff-44a5-9b16-2f1d5180b736
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 49103a8a-414b-4eb9-8b36-7a8c88acaa92
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 4b784ec2-5c14-483a-92d4-f6aa85c50f22
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message 682512bf-b6f1-444c-867b-ef3b3d8a3725
[92mINFO [0m:      Disconnect and shut down
🚀 Starting NASA FL Client: client_12
Algorithm: FEDAVG
Server: localhost:8686
K-Folds: 5
Log Directory: logs
🔄 Using device: cuda
🎯 GPU: NVIDIA A100-SXM4-80GB
🛑 Early stopping enabled:
   Patience: 3 epochs
   Min delta: 0.001
💾 Hyperparameters saved to: logs/client_12_hyperparams_20251030_111214.csv
🔍 Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.5/client_12
📊 Loaded 7920 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.5/client_12
🔢 Original feature dimension: 24
🔍 Applying KernelPCA with 5 components, kernel=poly
📊 Data shape - Before: (4752, 24), After: (4752, 5)
🔄 Created sequences with length 10
   Final dataset shape: X (4742, 10, 5), y (4742,)
✅ Data split completed:
   Training samples: 4742
   Validation samples: 1574
   Test samples: 1574
   Model type: lstm
   Final input dimension: 5
✅ Client client_12 ready:
   Model: LSTM
   Training: 4742 samples
   Device: cuda
   Validation: 1574 samples
   Test: 1574 samples
   Algorithm: fedavg
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs

🧪 Round 0 Evaluation Results:
   Test Loss: 8514.9290
   RMSE: 92.2764, MAE: 73.8355, R²: -0.5971
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv
⏩ Skipping CV for Round 2

🔄 Starting local training (Round 2)...
   Epoch 1/1: train_loss=6680.5375, val_rmse=72.7246 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.7246)
Round 2: epochs=1, train_loss=5410.4897, val_rmse=72.7246, val_r2=-0.0387
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 2 Evaluation Results:
   Test Loss: 5356.3128
   RMSE: 73.1868, MAE: 60.8353, R²: -0.0047
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv
⏩ Skipping CV for Round 3

🔄 Starting local training (Round 3)...
   Epoch 1/1: train_loss=6434.0109, val_rmse=72.2582 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.2582)

🎯 Round 3 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 5343.5859, RMSE: 73.0998, R²: -0.0243
   Validation - Loss: 5221.2437, RMSE: 72.2582, R²: -0.0255

🧪 Round 3 Evaluation Results:
   Test Loss: 5353.0903
   RMSE: 73.1648, MAE: 60.8224, R²: -0.0040
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv
⏩ Skipping CV for Round 4

🔄 Starting local training (Round 4)...
   Epoch 1/1: train_loss=6437.1515, val_rmse=72.0826 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.0826)
Round 4: epochs=1, train_loss=5318.5552, val_rmse=72.0826, val_r2=-0.0205
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 4 Evaluation Results:
   Test Loss: 5347.0825
   RMSE: 73.1237, MAE: 60.8002, R²: -0.0029
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv

🔍 Running 3-fold cross-validation (Round 5)

🔍 Starting 3-fold cross-validation on TRAINING data for client client_12

📊 Fold 1/3
   Fold 1 Results:
     Val Loss: 5395.2852
     Val RMSE: 73.4526, Val R²: -0.0146

📊 Fold 2/3
   Fold 2 Results:
     Val Loss: 5175.8389
     Val RMSE: 71.9433, Val R²: -0.0130

📊 Fold 3/3
   Fold 3 Results:
     Val Loss: 5370.9629
     Val RMSE: 73.2869, Val R²: -0.0285
💾 CV metrics saved to: logs/client_12_cv_metrics_20251030_111214.csv

📈 3-Fold CV Summary (Training Data):
   VAL_LOSS: 5314.0290 ± 98.2184
   RMSE: 72.8943 ± 0.6758
   R2: -0.0187 ± 0.0070

🔄 Starting local training (Round 5)...
   Epoch 1/1: train_loss=6372.2067, val_rmse=72.1492 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.1492)

🎯 Round 5 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 5328.0425, RMSE: 72.9934, R²: -0.0213
   Validation - Loss: 5205.5078, RMSE: 72.1492, R²: -0.0224
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 5 Evaluation Results:
   Test Loss: 5349.4574
   RMSE: 73.1400, MAE: 60.8094, R²: -0.0034
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv
⏩ Skipping CV for Round 6

🔄 Starting local training (Round 6)...
   Epoch 1/1: train_loss=6271.8116, val_rmse=72.6222 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.6222)

🎯 Round 6 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 5395.7500, RMSE: 73.4558, R²: -0.0343
   Validation - Loss: 5273.9775, RMSE: 72.6222, R²: -0.0358
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 6 Evaluation Results:
   Test Loss: 5346.3908
   RMSE: 73.1190, MAE: 60.7974, R²: -0.0028
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv
⏩ Skipping CV for Round 7

🔄 Starting local training (Round 7)...
   Epoch 1/1: train_loss=6295.2779, val_rmse=72.2321 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.2321)
Round 7: epochs=1, train_loss=5339.8706, val_rmse=72.2321, val_r2=-0.0247

🧪 Round 7 Evaluation Results:
   Test Loss: 5351.5694
   RMSE: 73.1544, MAE: 60.8172, R²: -0.0038
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv
⏩ Skipping CV for Round 8

🔄 Starting local training (Round 8)...
   Epoch 1/1: train_loss=6299.0022, val_rmse=72.1896 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.1896)
Round 8: epochs=1, train_loss=5333.8057, val_rmse=72.1896, val_r2=-0.0235
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 8 Evaluation Results:
   Test Loss: 5351.0113
   RMSE: 73.1506, MAE: 60.8152, R²: -0.0037
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv

🔍 Running 3-fold cross-validation (Round 9)
💾 CV metrics saved to: logs/client_12_cv_metrics_20251030_111214.csv

🔄 Starting local training (Round 9)...
   Epoch 1/1: train_loss=6371.9213, val_rmse=72.4378 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.4378)

🎯 Round 9 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 5369.2847, RMSE: 73.2754, R²: -0.0292
   Validation - Loss: 5247.2354, RMSE: 72.4378, R²: -0.0306
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 9 Evaluation Results:
   Test Loss: 5348.6633
   RMSE: 73.1346, MAE: 60.8064, R²: -0.0032
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv

🔍 Running 3-fold cross-validation (Round 10)
💾 CV metrics saved to: logs/client_12_cv_metrics_20251030_111214.csv

🔄 Starting local training (Round 10)...
   Epoch 1/1: train_loss=6257.5921, val_rmse=71.9768 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=71.9768)

🎯 Round 10 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 5303.5273, RMSE: 72.8253, R²: -0.0166
   Validation - Loss: 5180.6611, RMSE: 71.9768, R²: -0.0175
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111214.csv

🧪 Round 10 Evaluation Results:
   Test Loss: 5345.8452
   RMSE: 73.1153, MAE: 60.7951, R²: -0.0027
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111214.csv

================================================================================
🎯 FINAL COMPREHENSIVE REPORT
================================================================================
💾 Final summary saved to: logs/client_12_final_summary_20251030_111214.csv

📊 CLIENT: client_12 | ALGORITHM: fedavg | MODEL: LSTM
📈 TOTAL ROUNDS: 10

⚙️  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

🛑 EARLY STOPPING SUMMARY:
   Enabled: Yes
   Patience: 3 epochs
   Min delta: 0.001
   Total early stops: 0/10 rounds
   Early stop rate: 0.0%

🏁 FINAL ROUND PERFORMANCE:
   Training   - Loss:  5303.53 | RMSE:  72.83 | R²: -0.0166
   Validation - Loss:  5180.66 | RMSE:  71.98 | R²: -0.0175
   Test       - Loss:  5345.85 | RMSE:  73.12 | R²: -0.0027

📊 STATISTICS ACROSS ALL ROUNDS (Mean ± Std):
   Training Loss:    5351.35 ±  37.80
   Validation Loss:  5229.07 ±  38.24
   Test Loss:        5666.44 ± 949.50

   Training RMSE:    73.15 ±  0.26
   Validation RMSE:  72.31 ±  0.26
   Test RMSE:        75.06 ±  5.74

   Training R²:     -0.0257 ± 0.0072
   Validation R²:   -0.0270 ± 0.0075
   Test R²:         -0.0628 ± 0.1781

⭐ BEST PERFORMANCE:
   Best Round: 10 (Test R²: -0.0027)

📋 DATA SUMMARY:
   Training samples:   4742
   Validation samples: 1574
   Test samples:       1574
   Total samples:      7890
================================================================================
✅ Client client_12 completed | Algorithm: FEDAVG
