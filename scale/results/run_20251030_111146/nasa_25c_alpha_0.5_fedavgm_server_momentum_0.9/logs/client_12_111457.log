[93mWARNING [0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.
	Instead, use the `flower-supernode` CLI command to start a SuperNode as shown below:

		$ flower-supernode --insecure --superlink='<IP>:<PORT>'

	To view all available options, run:

		$ flower-supernode --help

	Using `start_client()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 71ecb48d-a32d-466d-b457-91a38b909acb
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message b610e6d4-2982-4d73-ab24-bd3c025fd6b3
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 75d5db65-5c16-4aaa-8e83-1574e57c97ec
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message ff18b018-1b08-4d4a-92be-ddf8122d486b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 769b8cdb-9666-483d-ba68-cf7940482a58
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 4bfd61c4-3ffc-4b9f-9695-1c6b9f0abdfe
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 14e65c8f-ac9d-458b-a1f8-42c0ec17baba
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message f1135984-ad5f-4cae-952c-ea28ff14275e
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message c8ec72ad-7580-424a-8d53-6d7b7259a5ba
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message ad09c4e7-790b-4d9a-93bb-703b1085831a
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 53f9a228-fa46-4956-89d4-3cb1f8183c9b
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message efd52b96-7fd4-41ea-b8bd-c4638cc8b1ad
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 0344a8cd-8755-4a2a-90bc-716233772074
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message c88e9175-fa06-4542-8f82-29ba3c319ed8
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 44583be8-dedd-4ff4-b96a-9a394a213987
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message f184f337-3584-47f8-91e3-afc24341aa57
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 07cf0082-c2b4-480b-a5ab-00769984613f
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: train message 62be0599-be9a-402b-8bb4-ece91857d327
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: evaluate message 217e6fd5-577a-4a6b-a5cd-df3f0d481c5e
[92mINFO [0m:      Sent reply
[92mINFO [0m:      
[92mINFO [0m:      Received: reconnect message c533d820-14e2-476b-aefb-49189315a4b3
[92mINFO [0m:      Disconnect and shut down
🚀 Starting NASA FL Client: client_12
Algorithm: FEDAVGM
Server: localhost:8688
K-Folds: 5
Log Directory: logs
🔄 Using device: cuda
🎯 GPU: NVIDIA A100-SXM4-80GB
🛑 Early stopping enabled:
   Patience: 3 epochs
   Min delta: 0.001
💾 Hyperparameters saved to: logs/client_12_hyperparams_20251030_111500.csv
🔍 Looking for data at: /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.5/client_12
📊 Loaded 7920 total samples from /mnt/ceph_drive/FL_IoT_Network/scale/data/nasa_cmaps/pre_split_data/25_clients/alpha_0.5/client_12
🔢 Original feature dimension: 24
🔍 Applying KernelPCA with 5 components, kernel=poly
📊 Data shape - Before: (4752, 24), After: (4752, 5)
🔄 Created sequences with length 10
   Final dataset shape: X (4742, 10, 5), y (4742,)
✅ Data split completed:
   Training samples: 4742
   Validation samples: 1574
   Test samples: 1574
   Model type: lstm
   Final input dimension: 5
✅ Client client_12 ready:
   Model: LSTM
   Training: 4742 samples
   Device: cuda
   Validation: 1574 samples
   Test: 1574 samples
   Algorithm: fedavgm
   Total Rounds: 10
   Learning Rate: 0.001
   Batch Size: 32
   Logging to: logs

🧪 Round 0 Evaluation Results:
   Test Loss: 7115.4521
   RMSE: 84.3531, MAE: 68.3202, R²: -0.3346
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv
⏩ Skipping CV for Round 2

🔄 Starting local training (Round 2)...
   Epoch 1/1: train_loss=6260.1632, val_rmse=72.1126 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.1126)
Round 2: epochs=1, train_loss=5322.8237, val_rmse=72.1126, val_r2=-0.0213
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 2 Evaluation Results:
   Test Loss: 140901.3433
   RMSE: 375.3683, MAE: 368.1981, R²: -25.4280
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv
⏩ Skipping CV for Round 3

🔄 Starting local training (Round 3)...
   Epoch 1/1: train_loss=27128.7557, val_rmse=72.2523 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=72.2523)

🎯 Round 3 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 5342.7388, RMSE: 73.0940, R²: -0.0241
   Validation - Loss: 5220.3872, RMSE: 72.2523, R²: -0.0253

🧪 Round 3 Evaluation Results:
   Test Loss: 299252.4951
   RMSE: 547.0398, MAE: 542.1448, R²: -55.1291
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv
⏩ Skipping CV for Round 4

🔄 Starting local training (Round 4)...
   Epoch 1/1: train_loss=64653.3362, val_rmse=94.7198 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=94.7198)
Round 4: epochs=1, train_loss=9114.0088, val_rmse=94.7198, val_r2=-0.7621
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 4 Evaluation Results:
   Test Loss: 33880.4952
   RMSE: 184.0666, MAE: 169.7967, R²: -5.3548
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv

🔍 Running 3-fold cross-validation (Round 5)

🔍 Starting 3-fold cross-validation on TRAINING data for client client_12

📊 Fold 1/3
   Fold 1 Results:
     Val Loss: 8485.3379
     Val RMSE: 92.1159, Val R²: -0.5957

📊 Fold 2/3
   Fold 2 Results:
     Val Loss: 8283.2803
     Val RMSE: 91.0125, Val R²: -0.6211

📊 Fold 3/3
   Fold 3 Results:
     Val Loss: 8296.4971
     Val RMSE: 91.0851, Val R²: -0.5887
💾 CV metrics saved to: logs/client_12_cv_metrics_20251030_111500.csv

📈 3-Fold CV Summary (Training Data):
   VAL_LOSS: 8355.0384 ± 92.2935
   RMSE: 91.4045 ± 0.5039
   R2: -0.6019 ± 0.0139

🔄 Starting local training (Round 5)...
   Epoch 1/1: train_loss=16683.8765, val_rmse=79.3865 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=79.3865)

🎯 Round 5 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 6436.9736, RMSE: 80.2308, R²: -0.2338
   Validation - Loss: 6302.2100, RMSE: 79.3865, R²: -0.2378
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 5 Evaluation Results:
   Test Loss: 10280.8499
   RMSE: 101.3945, MAE: 80.9373, R²: -0.9283
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv
⏩ Skipping CV for Round 6

🔄 Starting local training (Round 6)...
   Epoch 1/1: train_loss=9336.2629, val_rmse=88.8787 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=88.8787)

🎯 Round 6 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 8010.5659, RMSE: 89.5018, R²: -0.5355
   Validation - Loss: 7899.4150, RMSE: 88.8787, R²: -0.5514
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 6 Evaluation Results:
   Test Loss: 18842.0665
   RMSE: 137.2664, MAE: 116.2540, R²: -2.5341
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv
⏩ Skipping CV for Round 7

🔄 Starting local training (Round 7)...
   Epoch 1/1: train_loss=18192.9157, val_rmse=133.5052 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=133.5052)
Round 7: epochs=1, train_loss=17918.6934, val_rmse=133.5052, val_r2=-2.5006

🧪 Round 7 Evaluation Results:
   Test Loss: 19219.0566
   RMSE: 138.6328, MAE: 117.8454, R²: -2.6048
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv
⏩ Skipping CV for Round 8

🔄 Starting local training (Round 8)...
   Epoch 1/1: train_loss=18779.1968, val_rmse=136.4937 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=136.4937)
Round 8: epochs=1, train_loss=18724.6309, val_rmse=136.4937, val_r2=-2.6590
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 8 Evaluation Results:
   Test Loss: 19185.4177
   RMSE: 138.5114, MAE: 117.7029, R²: -2.5985
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv

🔍 Running 3-fold cross-validation (Round 9)
💾 CV metrics saved to: logs/client_12_cv_metrics_20251030_111500.csv

🔄 Starting local training (Round 9)...
   Epoch 1/1: train_loss=18745.7814, val_rmse=136.3710 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=136.3710)

🎯 Round 9 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 18691.1875, RMSE: 136.7157, R²: -2.5827
   Validation - Loss: 18597.0449, RMSE: 136.3710, R²: -2.6525
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 9 Evaluation Results:
   Test Loss: 19126.6787
   RMSE: 138.2992, MAE: 117.4553, R²: -2.5875
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv

🔍 Running 3-fold cross-validation (Round 10)
💾 CV metrics saved to: logs/client_12_cv_metrics_20251030_111500.csv

🔄 Starting local training (Round 10)...
   Epoch 1/1: train_loss=18687.6552, val_rmse=136.1577 ⭐ (improved by inf)
   ✅ Completed 1 epochs, using best model (val_rmse=136.1577)

🎯 Round 10 Training Summary:
   Epochs trained: 1/1
   Total early stops so far: 0
   Training   - Loss: 18633.1328, RMSE: 136.5032, R²: -2.5716
   Validation - Loss: 18538.9219, RMSE: 136.1577, R²: -2.6410
💾 Training metrics saved to: logs/client_12_training_metrics_20251030_111500.csv

🧪 Round 10 Evaluation Results:
   Test Loss: 19045.5667
   RMSE: 138.0057, MAE: 117.1126, R²: -2.5723
💾 Test metrics saved to: logs/client_12_test_metrics_20251030_111500.csv

================================================================================
🎯 FINAL COMPREHENSIVE REPORT
================================================================================
💾 Final summary saved to: logs/client_12_final_summary_20251030_111500.csv

📊 CLIENT: client_12 | ALGORITHM: fedavgm | MODEL: LSTM
📈 TOTAL ROUNDS: 10

⚙️  HYPERPARAMETERS:
   Learning Rate: 0.001
   Batch Size: 32
   Local Epochs: 1
   Hidden Dim: 64
   Num Layers: 2
   Sequence Length: 10
   Use Attention: False
   Dropout: 0.3
   Optimizer: adam

🛑 EARLY STOPPING SUMMARY:
   Enabled: Yes
   Patience: 3 epochs
   Min delta: 0.001
   Total early stops: 0/10 rounds
   Early stop rate: 0.0%

🏁 FINAL ROUND PERFORMANCE:
   Training   - Loss: 18633.13 | RMSE: 136.50 | R²: -2.5716
   Validation - Loss: 18538.92 | RMSE: 136.16 | R²: -2.6410
   Test       - Loss: 19045.57 | RMSE: 138.01 | R²: -2.5723

📊 STATISTICS ACROSS ALL ROUNDS (Mean ± Std):
   Training Loss:   12133.33 ± 5777.33
   Validation Loss: 12020.03 ± 5793.17
   Test Loss:       58684.94 ± 88369.27

   Training RMSE:   106.89 ± 26.61
   Validation RMSE: 106.30 ± 26.83
   Test RMSE:       198.29 ± 139.16

   Training R²:     -1.3257 ± 1.1074
   Validation R²:   -1.3607 ± 1.1378
   Test R²:         -10.0072 ± 16.5749

⭐ BEST PERFORMANCE:
   Best Round: 0 (Test R²: -0.3346)

📋 DATA SUMMARY:
   Training samples:   4742
   Validation samples: 1574
   Test samples:       1574
   Total samples:      7890
================================================================================
✅ Client client_12 completed | Algorithm: FEDAVGM
